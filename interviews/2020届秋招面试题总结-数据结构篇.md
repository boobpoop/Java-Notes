# 2020届秋招面试题总结2——数据结构篇

**1、说说常见的集合有哪些吧。**

Map接口和Collection接口是所有集合类框架的父接口：

Collcetion接口的子接口包括：Set接口和List接口。

- List接口的实现类主要有：ArrayList、LinkedList、Stack以及Vector，CopyOnWriteArrayList等。
- Set接口的实现类主要有：HashSet、TreeSet、LinkedHashSet等。
- Map接口的实现类主要有：HashMap、HashTable、ConcurrentHashMap以及Properties等。


![Y5FXy6.jpg](https://s1.ax1x.com/2020/05/19/Y5FXy6.jpg)


**2、ArrayList和LinkedList有什么区别。**

***2.2、ArrayList和LinkedList的区别。**#

答题要点：

-两者在读取和写的时间长短。
-两者的空间大小。
-list的初始化和扩容
-线程安全。


主要有以下几点区别：

- ArraryList是基于动态数组的数据结构，LinkeList实现了List和Deque接口，是基于链表的数据结果（LinkedList是双向链表，有next也有previous），LinkedList比ArrayList需要更多内存。
- 对于随机访问get和set，ArrayList优于LinkedList，因为LinkedList要移动指针。
- 对于新增和删除操作add和remove，LinkedList比较占优势，因为ArrayList要移动数据。

ArrayList默认容量是10，先扩容capacity的1.5倍，若扩容后还小于所需容量，那么令capacity=所需容量，若capacity超出int范围，抛异常。

Vector默认容量是10，扩容2倍，使用synchronized锁保证线程安全。

Collections.synchronizedList(list)基于装配器模式，包装方法全部用synchronized锁修饰。

***2.1、CopyOnWriteArrayList***

写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。 CopyOnWriteArrayList每次增删操作都是**新建数组操作，没有扩容机制**，由于增删改操作都是在新数组上进行，不会影响迭代器，因此是fail-safe。

写操作要加锁，防止并发写入时导致写入数据丢失，写操作结束之后，要把原始数组指向新的复制数组。
```
/**
    public boolean add(E e) {
        //使用ReentrantLock锁
        final ReentrantLock lock = this.lock;
        lock.lock();
        try {
            //复制数据
            Object[] elements = getArray();
            int len = elements.length;
            //插入数组
            Object[] newElements = Arrays.copyOf(elements, len + 1);
            newElements[len] = e;
            setArray(newElements);
            return true;
        } finally {
            lock.unlock();
        }
    }
    
    final void setArray(Object[] a) {
        array = a;
    }
```
读操作不需要加锁
```
public E get(int index) {
    return get(getArray(), index);
}
```

优点：

CopyOnWriteArrayList的并发安全性能比vector要好，vector是增删改查方法都加了synchronized关键字，但是每个方法执行的时候都要去获得锁，性能就会大大下降，而CopyOnWriteArrayList只是在增删改上加锁，**读不加锁（与ConcurrentHashMap一样）**，在读方面的性能好于vector，CopyOnWriteArrayList支持读多写少的并发情况。

缺点：
- 由于CopyOnWriteArrayList写操作的时候，需要拷贝数组，会消耗内存。如果原数组内容比较多的情况下，可能会导致minor gc(每次修改数组都会复制原数组，用完后丢弃)或者**full gc（数据过大放入老年代）**。
- 不能用于实时读写的场景，像拷贝数组、新增元素都需要时间。虽然CopyOnWriteArrayList 能做到最终一致性,但是还是**没法满足实时性要求**。



**3、用过哪些Map类，都有什么区别，HashMap时线程安全的吗，并发下使用的Map是什么，他们的内部原理分别是什么，比如存储方法，hashcode，扩容，默认容量等。**

map的thredshold = capacity * 0.75，也就是当元素不超过数组内存大小时，就会扩容，因此如果hash值分散，可以达成每个key-value均匀分布到table中。

结构：

主要用过HashMap，HashMap不是线程安全的，并发下使用的Map是ConcurrentHashMap，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的。

初始值：

HashMap中Node[] table的默认长度length是16，所能容纳的最大容量数据的Node（键值对）个数为threshold=length * loadfactor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。

扩容：

结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择。

具体扩容算法：

1. 当调用无参构造函数new HashMap()时，table默认的长度是16，loadFactor是0.75f，threshlod = 16 * 0.75 = 12,因此，当放第13个元素时，就会触发resize()对table进行加倍，threshold也会加倍。

2.当调用有参构造函数new HashMap(initCapacity)时，调用tableSizeFor()方法，给threshold赋值一个大于等于initCapacity的2次幂的值，此时table还是null。当put()时，如果table为空，进行resize()，此时将capacity=threshold，thredshold = capacity * loadFactor。
- 例如：在构造函数传入initCapcity=1000，通过tableSizeFor()后，capacity=thredshold=1024，此时调整thredshold=capacity * 0.75f = 768。即传入1000后，在put()第769个元素时，会进行扩容。因此，当放第threshold+1个元素时，就会触发resize()对table进行加倍，threshold也会加倍。
- 例如：在构造函数传入initCapcity=10000，通过tableSizeFor()后，capacity=thredshold=16384，此时调整thredshold=capacity * 0.75f = 12288。即传入10000后，在put()第12289个元素时，会进行扩容。

hash运算：

Hash算法本质上就是三步：取key的hashCode值，高位运算（当前hashcode ^（异或操作） hashcode >> 16），取模运算。注意，一般hashtable桶数都会选择素数，因为素数因子最少，能减少冲突。但是，hashmap却采用非常规方法，没有选用素数，而是选用合数，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。


***3.1 HashMap安全性问题***

HashMap不是同步的。当多线程并发访问一个哈希表时，需要在外部进行同步操作，否则会引发数据不同步问题。

当我们需要一个同步的HashMap时，有两种选择：

- 选择全局加锁，考虑用Collections.synchronizedMap包一层锁，变成个线程安全的Map：Map m = Collections.synchronizedMap(new HashMap(...));
- 使用ConcurrentHashMap。这两个选项之间的首选是使用ConcurrentHashMap，这是因为我们不需要锁定整个对象。

Collections.synchronizedMap：在hashmap基础上，在方法内部加一个锁（在构造函数中初始化为SynchronizedMap类本身），每次调用get、put等方法时都用synchronized方法块来实现同步功能。代码如下：


```java
private static class SynchronizedMap<K,V>
    implements Map<K,V>, Serializable {
    private static final long serialVersionUID = 1978198479659022715L;
    private final Map<K,V> m;     // Backing Map
    final Object      mutex;        // Object on which to synchronize
    SynchronizedMap(Map<K,V> m) {
        this.m = Objects.requireNonNull(m);
        mutex = this;
    }
    SynchronizedMap(Map<K,V> m, Object mutex) {
        this.m = m;
        this.mutex = mutex;
    }
    public int size() {
        synchronized (mutex) {return m.size();}
    }
    public V get(Object key) {
        synchronized (mutex) {return m.get(key);}
    }
    public V put(K key, V value) {
        synchronized (mutex) {return m.put(key, value);}
    }
    public V remove(Object key) {
        synchronized (mutex) {return m.remove(key);}
    }
    // 省略其他方法
}
```

**4、HashMap与HashTable的区别。**

主要有以下几点区别。

- HashMap没有考虑同步，是线程不安全的；HashTable在关键方法（put、get、contains、size等）上使用了Synchronized关键字，是线程安全的。
- HashMap允许Key/Value都为null，后者Key/Value都不允许为null。
- 在jdk1.8中，HashMap的底层结构是数组+链表+红黑树，而HashTable的底层结构是数组+链表。
- HashMap中数组的默认初始容量是16，并且必须是2的指数倍数，扩容时`newCapacity=2*oldCapacity；`而HashTable中默认的初始容量是11，并且不要求必须是2的指数倍数，扩容时`newCapacity=2*oldCapacity+1`。因此hashtable在计算下标时，效率也很低。
- 在hash取模计算时，HashTable的模数一般为素数，简单的做除取模结果会更为均匀，`int index = (hash & 0x7FFFFFFF) % tab.length;`
- HashMap的模数为2的幂，直接用位运算来得到结果，效率要大大高于做除法，`i = (n - 1) & hash；

**5、HashMap的put方法的具体流程。**

-1.看table是否为空，为空就resize()
1.计算下标。
2.是否hash冲突。
3.key是否存在，如果存在就用新value覆盖旧value，否则在尾巴上新建一个结点。
4.链表是否大于8，转成红黑树。
5.元素个数是否大于thredshold，是的话进行resize()扩容。

在jdk1.8以前的HashMap的实现是数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。当HashMap中有大量的元素都存放在同一个桶中时，这个桶下有一条长长的链表，这个时候HashMap就相当于一个单链表，假如单链表有n个元素，遍历的时间复杂度就是O(n)，完全失去了它的优势。

针对这种情况，JDK1.8引入了红黑树（查找时间复杂度为O(logn)）来优化这个问题。

下面具体看put方法的源码。

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    // 1 如果table为空或者长度为0，即没有元素，则使用resize()方法扩容
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    // 2 计算插入存储的数组索引i，如果数组当前位置为空，则不存在Hash冲突，可以直接插入元素。
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    // 3 插入新元素时，如果发生Hash冲突，则依次往下判断。
    else {
        Node<K,V> e; K k;
        // 3.1 判断table[i]的元素的key是否与需要插入的key相同，若相同则直接用新的value覆盖掉旧的value，判断元素相等原则是equals()，所以key对象需要重写该方法。
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        // 3.2 判断需要插入的数据结构是红黑树还是链表，如果是红黑树，则直接在树中通过putTreeVal()方法插入/更新键值对。
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        // 3.3 如果是链表，则在链表中插入/更新键值对
        else {
            // 3.3.1 遍历table[i]，判断key是否已经存在，采用equals对比当前遍历结点的key与需要插入数据的key，如果存在相同，则直接覆盖。
            // 3.3.2 遍历完毕后如果没有发现相同的key，直接在链表尾部插入新的节点元素。
            // 插入完成后判断链表长度是否>TREEIFY_THRESHOLD（8），若是，则把链表转换为红黑树。
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        // 3.4  如果e不为空，证明key已经存在，直接用新的value覆盖旧的value即可。
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    // 4 插入成功后，判断实际存在的键值对数量size>最大容量，如果大于则进行扩容。
    if (++size > threshold)
        resize();
    // 5 插入成功时会调用的方法（默认实现为空）
    afterNodeInsertion(evict);
    return null;
}
```

简单的图片总结为：

![Y5k8XV.png](https://s1.ax1x.com/2020/05/19/Y5k8XV.png)

HashMap中的红黑树操作具体可以参考：[Java 集合深入理解（17）：HashMap 在 JDK 1.8 后新增的红黑树结构](https://blog.csdn.net/u011240877/article/details/53358305)

**6、HashMap的扩容操作是怎么实现的。**

***注意HashMap是懒加载的***，当要插入元素时，才会建立数组。在HashMap中，通过resize()方法来进行扩容或者初始化操作。直接上源码。

```java
final Node<K,V>[] resize() {
        // 1 复制一份扩容前的数组（当前数组）
    Node<K,V>[] oldTab = table;
        // 2 保存旧的数组长度，阈值
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
        // 3 判断扩容前的数组容量
    if (oldCap > 0) {
        // 3.1 若扩容前的数组容量超过最大值，则不再扩容。
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        // 3.2 若没有超过最大值，则扩容为原来二倍。
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    // 4 如果旧容量为0，并且旧阈值>0，说明之前创建了哈希表但没有添加元素，初始化容量等于阈值。
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    // 5 如果旧容量、旧阈值都为0，说明还没创建哈希表，容量为默认值，阈值为容量*加载因子。
    else {               // zero initial threshold signifies using defaults
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
        // 6 如果新的阈值为0，则用新容量*加载因子 重新计算一次。
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    // 7 更新阈值。
    threshold = newThr;
    // 8 创建新链表数组，容量是原来两倍。
    @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    // 9 接下来是遍历复制。
    if (oldTab != null) {
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                // 9.1 旧的桶置为空
                oldTab[j] = null;
                // 9.2 如果当前桶只有一个元素，则直接赋值给新table的对应位置。
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                // 9.3 如果当前桶是树形结构，则要把新table当前位置桶也变成树结构。
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                // 9.4 保留旧哈希表桶中链表元素的顺序
                else { // preserve order
                    Node<K,V> loHead = null, loTail = null;//***按原始链表顺序，过滤出来扩容后位置不变的元素（低位=0），放在一起。***
                    Node<K,V> hiHead = null, hiTail = null;//按原始链表顺序，过滤出来扩容后位置改变到（index+oldCap）的元素（高位=0），放在一起。
                    Node<K,V> next;
                    // 9.4.1 do-while循环赋值给新哈希表
                    do {
                        next = e.next;
                        // 9.4.2 通过计算(e.hash & oldCap) == 0分成两条链表，再将两条链表散列到新数组的不同位置。
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    // 9.4.3 放到原索引位置不变的桶中。
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    // 9.4.4 放到原索引+oldCap位置的桶中。
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

有一点注意区别，JDK1.7中resize的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但在JDK1.8不会倒置，链表元素还是按照之前顺序。

**7、HashMap是怎么解决哈希冲突的。**

哈希冲突：当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做哈希碰撞。

- 使用链地址法（散列表）来链接拥有相同hash值的元素；
- 使用2次扰动（hash函数）来降低哈希冲突的概率，使得概率分布更为均匀；
- 引入红黑树进一步降低遍历的时间复杂度。



![Y5kv3n.png](https://s1.ax1x.com/2020/05/19/Y5kv3n.png)


**hashCode取值出的高位也参与运算，进一步降低hash碰撞的概率，使得数据分布更平均，我们把这样的操作称为扰动，** 在JDK 1.8中的hash()函数如下：

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```




**8、HashMap为什么不直接使用hashCode()处理后的哈希值之直接作为table的下标。**

hashCode太大，超过数组允许的大小。

所以解决方法为：
- 在HashMap中实现了自己的hash()方法，通过两次扰动使得元素对象的哈希值高低位自行进行异或运算，降低哈希碰撞概率也使得数据分布更平均。
- 在保证数组长度为2的幂次方的时候，使用hash()运算之后的值与运算（&）（数组长度 - 1）来获取数组下标的方式进行存储，这样一来是比取余操作更加有效率，二来也是因为只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，三来解决了“哈希值与数组大小范围不匹配”的问题。

为什么数组长度要保证2的幂次方：

- 只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，可以用位运算来代替做除取模运算，实现了key的定位，2的幂次方也可以减少冲突次数，提高HashMap的查询效率；
- 如果 length 为 2 的次幂 则 length-1 转化为二进制必定是 11111……的形式，在于 h 的二进制与操作效率会非常的快，而且空间不浪费；如果 length 不是 2 的次幂，比如 length 为 15，则 length - 1 为 14，对应的二进制为 1110，在于 h 与操作，最后一位都为 0 ，而 0001，0011，0101，1001，1011，0111，1101 这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！这样就会造成空间的浪费。
- 当然，HashTable就没有采用2的幂作为数组长度，而是采用素数。素数的话是用简单做除取模方法来获取下标index，而不是位运算，效率低了不少，但分布也很均匀。

**9、为什么HashMap中String、Integer这样的包装类适合作为Key。**

String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率。

- 都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况。
- 内部已经重写equals()、hashCode()等方法，遵守了HashMap内部的规范，不容易出现Hash值计算错误的情况。

如果想要自己写的类作为Key，需要重写hashCode()和equals()方法。

- 重写HashCode()是因为需要计算存储数据的存储位置，需要注意，不要试图从散列码计算中排除掉一个对象的关键部分来提高性能，这样虽然能更快但是可能会导致更多的Hash碰撞。
- 重写equals()方法，需要遵守自反性、对称性、传递性、一致性以及对于任务非null的引用之x，x.equals(null)必须返回false这几个特性，目的就是为了保证Key在哈希表中的唯一性。


**10、ConcurrentHashMap和HashMap的区别。**

ConcurrentHashMap结合了HashMap和HashTable二者的优势。HashMap没有考虑同步，HashTable却要在每次同此执行时都要锁住整个结构（用Synchronized关键字）。ConcurrentHashMap锁的方式是稍微细粒度的。

在JDK 1.8中，ConcurrentHashMap放弃了臃肿的分段锁设计，取而代之的是采用Node+CAS+Synchronized来保证并发安全进行实现。

CAS：如果数组档期那位置没有元素，则通过CAS自旋锁的方式插入元素。
Synchornized：如果数组当前位置有元素，则通过Synchronized方式锁着数组当前元素所对应的整个链表。当前链表元素大于8，通过红黑树对链表进行插入。
Node：对应整个链表，相对于1.7的分段锁，对Node加锁粒度更细。

volatile：出了数组是volatile，每个value也是volatile，这样每次获取数组或者数组元素时，会拿最新的数据。

以插入元素（put方法）为例。

```java
public V put(K key, V value) {
    return putVal(key, value, false);
}
final V putVal(K key, V value, boolean onlyIfAbsent) {
    // 1 key和value不能为null。
    if (key == null || value == null) throw new NullPointerException();
    // 2 通过spread函数获取hash值（与HashMap类似，也加入了扰动）
    int hash = spread(key.hashCode());
    int binCount = 0;//用来计算在这个桶总共有多少元素，用来控制扩容或者转换为红黑树。
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh;
        // 3 table为空，则先初始化数组。
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        // 4 如果table对应位置为空，则调用CAS插入相应的数据，注意这个地方没有加锁。
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;                   // no lock when adding to empty bin
        }
        // 5 如果取出来的节点的hash值是MOVED(-1)的话，则表示当前正在对这个数组进行扩容，复制到新的数组，则当前线程也去帮助复制。
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            // 6 如果这个位置有元素的话，则对该节点加synchronized锁。
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    // 7 如果是链表的话(hash大于0)，就对这个链表的所有元素进行遍历。
                    if (fh >= 0) {
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            // 8 若找到Key和Key的hash值都一样的节点，则用新的value值替换旧值。
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            // 9 若没有找到相同Key节点的话，则在链表后面添加新的节点。
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    // 10 当前桶的数据结构为红黑树时，调用putTreeVal方法将元素添加到树中。
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
            }
            // 11 当在同一个节点的数目达到TREEIFY_THRESHOLD(8)个的时候，则扩张数组或将给节点的数据转为树。
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    // 12 执行addCount()方法尝试更新元素个数baseCount。
    addCount(1L, binCount);
    return null;
}
```


**11、HashMap为什么选用红黑树。**

HashMap在jdk1.8之后引入了红黑树的概念，表示若桶中链表元素超过8时，会自动转化成红黑树；若桶中元素小于等于6时，树结构还原成链表形式。

因为红黑树的平均查找长度是log(n)，长度为8的时候，平均查找长度为3，如果继续使用链表，平均查找长度为8/2=4，这才有转换为树的必要。链表长度如果是小于等于6，6/2=3，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短。

并且，红黑树不像平衡树那样追求绝对的平衡，省去了很多没有必要的调平衡操作，插入删除元素等操作效率提高了很多。

另外，上下阈值选择6和8的情况下，中间有个差值7可以防止链表和树之间频繁的转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。

**12、Java8的ConcurrentHashMap为什么放弃了分段锁，有什么问题吗，如果你来设计，你如何设计。**

生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。jdk8放弃了分段锁而采用了Node锁，降低了锁的粒度，提高了性能，并使用CAS操作来确保Node的一些操作的原子性，取代了锁。

但是，ConcurrentHashMap的一些操作使用了synchronized锁，而不是ReentrantLock，虽然说jdk8中对synchronized进行了性能优化，但是我觉得使用ReentrantLock锁能更多的提高性能。

ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。

Synchronized是悲观锁，在jdk1.8之后，加入了偏向锁，轻量级锁（自旋锁），性能得到了极大优化。

上述两种锁都是可重入锁，在锁的细粒度和灵活度方面，很明显ReenTrantLock优于Synchronized。

**13、ConcurrentHashMap为何读不加锁。**

ConcurrentHashMap中value，Node，table都加了volatile关键字，保持可见性。

由于读写操作同时进行，也不会导致内存中数据不安全。由于ConcurrentHashMap加了volatile关键字修饰，保持可见性，使得数据一经修改，get时就会从主存中获取最新数据。


table加锁

在JDK 1.8中，ConcurrentHashMap采用Node+CAS+Synchronized来保证并发安全，在读操作时没有加锁操作。先看一下ConcurrentHashMap读操作的相关源码。

```java
transient volatile Node<K,V>[] table;//代表table的地址是volatile，而不是数组元素是volatile。这处是为了使得table在扩容时对其他线程也保持可见性。
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    //用volatile修饰Node的元素val和指针next，在多线程环境下保持可见性。
    volatile V val;
    volatile Node<K,V> next;
    Node(int hash, K key, V val, Node<K,V> next) {
        this.hash = hash;
        this.key = key;
        this.val = val;
        this.next = next;
    }
    //省略其他代码
}
//在读操作中没有一处加锁。
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    int h = spread(key.hashCode());//计算hash值
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (e = tabAt(tab, (n - 1) & h)) != null) {//读取首节点的Node元素
        if ((eh = e.hash) == h) {//如果Key相等则直接返回首节点
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        // eh（hash值）为负值表示正在扩容，这个时候查的是ForwardingNode的find方法来定位到nextTable。
        // eh=-1，说明该节点是一个ForwardingNode，正在迁移，此时调用ForwardingNode的find方法去nextTable里找。
        // eh=-2，说明该节点是一个TreeBin，此时调用TreeBin的find方法遍历红黑树，由于红黑树有可能正在旋转变色，所以find里会有读写锁。
        // eh>=0，说明该节点下挂的是一个链表，直接遍历该链表即可。

        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        while ((e = e.next) != null) {//直接在链表遍历查询
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

总结：

- 在JDK 1.8中，ConcurrentHashMap的get操作不需要加锁，这也是它比其他并发集合比如HashTable、用Collections.synchronizedMap()包装的HashMap更为安全高效的原因。
- get操作全程不需要加锁是因为Node的成员val是用volatile修饰的，**和数组用volatile修饰没有关系**。
- 数组用volatile修饰主要是保证数组在扩容的时候保持可见性。

[一篇volatile最棒的文章](https://www.cnblogs.com/keeya/p/9255136.html)


**10、LinkedHashMap**
LinkedHashMap出了继承了HashMap，内部新增了表头和表尾的指针，由于维护结点插入时的顺序。

***10、LinkedHashMap编写LRU***
```
Class LRUCache<K, V> extends LinkedHashMap<K, V>{
    
    //最大缓存空间为3
    private static final int MAX_ENTRIES = 3;
    
    //当结点数大于3时，删除最近最久未使用
    protected boolean removeOldestEntry(Map.Entry eldst){
        return size() > MAX_ENTRIES;
    }
    
    LRUChae(){
        //最大缓存空间，加载因子（扩容用），开启LRU
        super(MAX_ENTRIES, 0.75f, true);
    }
}
```


**11、Java集合的快速失败机制“fail-fast”，以及安全失败“fail-safe”。**

“fail-fast”是java集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。

例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。

原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常ConcurrentModification，终止遍历。

来看一下ArrayList源码部分，在next方法执行时，会先执行checkForComodification方法来检查。

```java
public E next() {
    checkForComodification();
    int i = cursor;
    if (i >= size)
        throw new NoSuchElementException();
    Object[] elementData = ArrayList.this.elementData;
    if (i >= elementData.length)
        throw new ConcurrentModificationException();
    cursor = i + 1;
    return (E) elementData[lastRet = i];
}
final void checkForComodification() {
    if (modCount != expectedModCount)
        throw new ConcurrentModificationException();
}
```

java.util包下的集合类都是快速失败机制，不能在多线程下发生并修改（迭代过程中被修改）。

解决办法：

- 1在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。
- 使用CopyOnWriteArrayList来替换ArrayList。

**与“fail-fast”对应的是“fail-safe”。**

采用安全失败机制的集合容器,在遍历时不是直接在集合内容上访问的,而是先copy原有集合内容,在拷贝的集合上进行遍历。由于迭代时是对原集合的拷贝的值进行遍历,所以在遍历过程中对原集合所作的修改并不能被迭代器检测到,所以不会触发ConcurrentModificationException异常。

基于拷贝内容的迭代虽然避免了ConcurrentModificationException异常，但同样地，迭代器并不能访问到修改后的内容，简单来说，迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器行为是不知道的。

java.util.concurrent包下的容器都是安全失败的,可以在多线程下并发使用,并发修改。


**14、HashSet是如何保证数据不可重复的。**

HashSet的底层其实就是HashMap，只不过HashSet是实现了Set接口，并且把数据作为Key值，而Value值一直使用一个相同的虚值来保存。

```java
private transient HashMap<E,Object> map;//底层实现的HashMap
private static final Object PRESENT = new Object();//虚值
public HashSet() {
    map = new HashMap<>();
}
public boolean add(E e) {
    return map.put(e, PRESENT)==null;//如果插入重复Value，map会返回上次插入的旧值PRENET，就不为null，这句话就返回false，代表插入失败。
}
```

由于HashMap的K值本身就不允许重复，并且在HashMap中如果K/V相同时，会用新的V覆盖掉旧的V，然后返回旧的V，那么在HashSet中执行这一句话始终会返回一个false，导致插入失败，这样就保证了数据的不可重复性。

**15、BlockingQueue是什么。**


java.util.concurrent.BlockingQueue是一个队列，在进行检索或移除一个元素的时候，它会等待队列变为非空；当添加一个元素时，它会等待队列中的可用空间。

BlockingQueue接口是java集合框架的一部分，主要用于实现生产者-消费者模式。这样我们就不需要担心等待生产者有可用的空间，以及消费者有可用的对象。因为它们都在BlockingQueue的实现类中被处理了。

Java提供了几种BlockingQueue的实现，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue,、SynchronousQueue等。


**16、Iterator中是否存在Add方法。**

Iterator是没有add方法的，但是有remove方法。Iterator.remove()是唯一安全的方式来在迭代过程中修改集合；如果在迭代过程中以任何其它的方式修改了基本集合将会产生未知的行为。而且每调用一次next()方法，remove()方法只能被调用一次，如果违反这个规则将抛出一个异常。

ListIterator是一个功能更强大的迭代器，继承于Iterator，可以通过调用listIterator()方法产生一个指向List开始处的ListIterator。它内部实现了一个双向链表，是具有add方法的。

两者的主要区别如下：

- ListIterator有add()方法，可以在遍历的时候向List中添加对象，而Iterator不可以。
- ListIterator和Iterator都有hasNext()和next()方法，可以实现顺序往后遍历，但是ListIterator有hasPrevious()和previous()方法，可以实现逆向遍历，Iterator就不可以。
- ListIterator可以定位当前的索引位置，nextIndex()和previousIndex()可以实现。Iterator没有这个功能。
- 都可以实现删除对象，但是ListIterator可以实现对象的修改，set()方法可以实现。Iterator仅能遍历，不能修改。



**17、有没有有顺序的Map实现类，如果有，他们怎么保证有序。**

顺序的Map实现类：LinkedHashMap，TreeMap

- LinkedHashMap是基于元素进入集合的顺序或者被访问的先后顺序排序。
- TreeMap则是基于元素的固有顺序（由Comparator或者Comarable确定）。TreeMap底层使用红黑树实现。
